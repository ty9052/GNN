{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFENpdxWAcei"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Download the corresponding PyTorch Geometric module\n",
        "\"\"\"\n",
        "Assign to TORCH with what you get from the cell above. E.g., export TORCH=1.12.1+cu113\n",
        "\"\"\"\n",
        "%env TORCH=1.13.0+cu116\n",
        "!pip install scipy==1.8.0\n",
        "!pip install networkx\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTfTY7PqAfI2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import Sequential, GCNConv, global_mean_pool\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "rng = np.random.default_rng()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LH1kRDEAg_E",
        "outputId": "7c757e08-823c-42fd-912d-5affa52ae129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
          ]
        }
      ],
      "source": [
        "# The range of different graph sizes\n",
        "global graph_size\n",
        "graph_size = 15\n",
        "graph_depth = int((graph_size)**(1/2))\n",
        "module = 7\n",
        "# The list of all graph pairs\n",
        "graphs = []\n",
        "\n",
        "from networkx.generators.nonisomorphic_trees import nonisomorphic_trees\n",
        "#tree_list = list(nonisomorphic_trees(graph_size))\n",
        "tree_list = [nx.balanced_tree(r=2, h=graph_depth)]\n",
        "print(tree_list[0].nodes)\n",
        "di_tree_list = []\n",
        "for G in tree_list:\n",
        "  di_tree_list.append(nx.DiGraph([(u,v) for (u,v) in G.edges() if u<v]+[(v,u) for (u,v) in G.edges() if u>v]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeYdP0Q1Ailb"
      },
      "outputs": [],
      "source": [
        "def sum_tree(node, gg, t):\n",
        "  s = 0\n",
        "  has_neighbor = False\n",
        "  for n in list(t.neighbors(node)):\n",
        "    if not has_neighbor:\n",
        "      has_neighbor = True\n",
        "    if gg.x[n] != gg.x[node]:\n",
        "      s += 1.0*sum_tree(n,gg,t)\n",
        "    #elif gg.x[n] != gg.x[node]:\n",
        "      #s += 2.0*sum_tree(n,gg,t)\n",
        "  if has_neighbor:\n",
        "    return s\n",
        "  else:\n",
        "    return 1.0\n",
        "\n",
        "def sum_diff_tree(node, gg, t):\n",
        "  s = 0\n",
        "  has_neighbor = False\n",
        "  for n in list(t.neighbors(node)):\n",
        "    #print(list(t.neighbors(node)))\n",
        "    if not has_neighbor:\n",
        "      has_neighbor = True\n",
        "    if gg.x[n] == 1.0:\n",
        "    #if True:\n",
        "      s += sum_tree(n,gg,t)\n",
        "  if has_neighbor:\n",
        "    return s\n",
        "  else:\n",
        "    return 1.0\n",
        "\n",
        "def sum_1_tree(node, gg, t):\n",
        "  for n in list(t.neighbors(node)):\n",
        "    if gg.x[n] == gg.x[0]:\n",
        "      return 1\n",
        "  return 0\n",
        "\n",
        "def sum_s_tree(node, gg, t):\n",
        "  s = 0\n",
        "  has_neighbor = False\n",
        "  for n in list(t.neighbors(node)):\n",
        "    if not has_neighbor:\n",
        "      has_neighbor = True\n",
        "    s += sum_tree(n,gg,t)\n",
        "  if has_neighbor:\n",
        "    return s\n",
        "  else:\n",
        "    return gg.x[node]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DmsqsrTAj87",
        "outputId": "9b5eb794-fffa-4965-fea1-2e31bdb042ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32768\n",
            "<class 'torch.Tensor'> tensor([0])\n",
            "Data(edge_index=[2, 14], num_nodes=15, x=[15, 1], y=[1]) tensor([0])\n",
            "tensor([8])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric\n",
        "# The list of Data objects\n",
        "dataset = []\n",
        "maximum = 0\n",
        "# Loop over trees\n",
        "for tree in di_tree_list:\n",
        "  #for i in range(graph_size+1):\n",
        "    #graph_geom.x = (torch.rand(size=(graph_geom.num_nodes,1)) < 0.5).float()\n",
        "  for j in range(2**(graph_size)):\n",
        "    graph_geom = torch_geometric.utils.from_networkx(tree)\n",
        "    node_feats = list(map(float, list('{0:0b}'.format(j))))\n",
        "    if len(node_feats)<graph_size:\n",
        "      node_feats+=[0 for k in range(graph_size-len(node_feats))]\n",
        "    #print(node_feats)\n",
        "    graph_geom.x = torch.unsqueeze(torch.tensor(node_feats), 1)\n",
        "    #print(graph_geom.x)\n",
        "    #print(graph_geom.x)\n",
        "    #graph_geom.x.requires_grad_()\n",
        "    #graph_geom.y = graph_geom.x[0]\n",
        "    graph_geom.y = torch.tensor([int(sum_tree(0, graph_geom, tree))])\n",
        "    # Add the Data object to the dataset\n",
        "    dataset.append(graph_geom.to(device))\n",
        "    #graph_geom.y.requires_grad_()\n",
        "\n",
        "    if graph_geom.y>maximum:\n",
        "      maximum=graph_geom.y\n",
        "\n",
        "      \n",
        "print(len(dataset))\n",
        "print(type(dataset[0].x[0][0]), dataset[0].y)\n",
        "print(dataset[1000], dataset[1].y)\n",
        "print(maximum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-SEKGjPAYSP"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GAT\n",
        "\n",
        "class GAT_N(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer = GAT(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk=None\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GAT) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)\n",
        "\n",
        "class GAT_CAT(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer = GAT(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk='cat'\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GAT) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)\n",
        "\n",
        "class GAT_MAX(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer=GAT(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk='max'\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GAT) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK53zt6l8Q0m"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCN\n",
        "\n",
        "class GCN_N(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer = GCN(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk=None\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GCN) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)\n",
        "\n",
        "class GCN_CAT(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer = GCN(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk='cat'\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GCN) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)\n",
        "\n",
        "class GCN_MAX(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, num_layers=0, graph_size=graph_size, act_fn=nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.act_fn = act_fn\n",
        "    self.num_layers = num_layers\n",
        "    self.layers = nn.ModuleList([])\n",
        "    #self.layers.append(nn.Embedding(2, hidden_dim))\n",
        "    self.layer=GCN(\n",
        "        input_dim, hidden_dim, num_layers, output_dim, v2=True, jk='max'\n",
        "    )\n",
        "    #self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "  def forward(self, input_geom):\n",
        "    x = input_geom.x\n",
        "    e = input_geom.edge_index\n",
        "    #x = self.layers[0](x.flatten())\n",
        "    x = self.layer(x,e[[1,0],:])\n",
        "    #x = self.layers[-1](x)\n",
        "    return x\n",
        "    #return nn.LogSoftmax(dim=1)(x)\n",
        "    \n",
        "  @staticmethod\n",
        "  def _weight_reset(module):\n",
        "    if isinstance(module, GCN) or isinstance(module, nn.Linear):\n",
        "      module.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    return self.layers.apply(type(self)._weight_reset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oU1Ei4nAl6L"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn.norm import graph_size_norm\n",
        "from operator import length_hint\n",
        "def test(dataloader, model):\n",
        "    \"\"\"Test a model on some data.\"\"\"\n",
        "    global graph_size\n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Get the number of datapoints\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    # The number of correct predictions\n",
        "    correct = 0\n",
        "\n",
        "    # We don't want to be computing the gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Loop through the minibatches\n",
        "        for data in dataloader:\n",
        "\n",
        "            # Compute the model predictions\n",
        "            result = model(data)\n",
        "            length,dim = result.shape\n",
        "            batch_size = int(length/graph_size)\n",
        "            pred = torch.zeros((batch_size,dim))\n",
        "            \n",
        "            for i in range(length):\n",
        "              if i % graph_size == 0:\n",
        "                pred[0] = result[0]\n",
        "            pred = result[torch.arange(batch_size)*graph_size]\n",
        "            '''\n",
        "            # Update with the number of correct predictions\n",
        "            for i in range(batch_size):\n",
        "              if data.y[i]+0.5> pred[i] and data.y[i]-0.5 <= pred[i]:\n",
        "                correct += 1\n",
        "            '''\n",
        "            correct += (pred.argmax(dim=1)==data.y).sum()\n",
        "            '''\n",
        "            for i in range(batch_size):\n",
        "              if pred[i] == data.y[i]:\n",
        "                correct+=1\n",
        "            '''\n",
        "    # Compute the accuracy for the whole dataset and return it\n",
        "    return correct / len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35iDzzGVArnB"
      },
      "outputs": [],
      "source": [
        "def eval(dataset, model, loss_fn, optimiser, max_patience=5, num_splits=5,\n",
        "                   batch_size=32, epochs=200, output_every=100, early_stop = 10):\n",
        "\n",
        "  # Get the number of graphs\n",
        "  size = len(dataset)\n",
        "\n",
        "  # Construct a permuter which keeps paired graphs together\n",
        "  graph_permuter = rng.permutation(np.arange(size))\n",
        "  #graph_permuter = np.arange(size)\n",
        "  \n",
        "  # Use the permuter to shuffle the dataset\n",
        "  shuffled_dataset = []\n",
        "  for i in graph_permuter:\n",
        "      shuffled_dataset.append(dataset[i])\n",
        "\n",
        "  train_dataset = shuffled_dataset[:int(0.8*size)]\n",
        "  test_dataset = shuffled_dataset[int(0.8*size):int(0.9*size)]\n",
        "  valid_dataset = shuffled_dataset[int(0.9*size):]\n",
        "\n",
        "  \n",
        "  # Reset the parameter of the model before training\n",
        "  model.reset_parameters()\n",
        "\n",
        "  model.train()\n",
        "  losses = []\n",
        "  train_accuracies = []\n",
        "  test_accuracies = []\n",
        "  val_accuracies = []\n",
        "  streaks = 0\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "  valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "  for epoch in range(epochs):\n",
        "    temp_loss = []\n",
        "    for data in train_dataloader:\n",
        "      optimizer.zero_grad()\n",
        "      result = model(data)\n",
        "      length, dim = result.shape\n",
        "      batch_size = int(length/graph_size)\n",
        "      # Compute the loss for this prediction\n",
        "      pred = torch.zeros((batch_size,dim))\n",
        "      #print(pred[0:10])\n",
        "      if batch_size == 1:\n",
        "        pred[0] = result[0]\n",
        "      else:\n",
        "        for i in range(length):\n",
        "          if i % graph_size == 0:\n",
        "            pred[int(i/graph_size)] = result[i]\n",
        "      \n",
        "      pred = result[torch.arange(batch_size)*graph_size]\n",
        "      #pred.requires_grad_()\n",
        "      loss = loss_fn(pred, data.y)\n",
        "      #result.requires_grad_()\n",
        "      #print(loss.requires_grad)\n",
        "      #print(result.requires_grad)\n",
        "      #print(pred.requires_grad)\n",
        "      loss.requires_grad_()\n",
        "      loss.backward()\n",
        "      #print(loss.is_leaf)\n",
        "      \n",
        "      #print(loss.grad)\n",
        "      #print(loss)\n",
        "      #print(result.grad)\n",
        "      optimizer.step()\n",
        "\n",
        "      temp_loss.append(loss.item())\n",
        "    losses.append(sum(temp_loss)/len(temp_loss))\n",
        "    train_acc = test(train_dataloader, model)\n",
        "    test_acc = test(test_dataloader, model)\n",
        "    val_acc = test(valid_dataloader, model)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    pickle1 = open(\"X.pickle\",\"wb\")\n",
        "    pickle.dump([train_accuracies,test_accuracies,val_accuracies],pickle1)\n",
        "    pickle1.close()\n",
        "    if epoch % output_every ==0:\n",
        "      print(\"===============Epoch\"+str(epoch)+\"==============\")\n",
        "      print(pred[0:5])\n",
        "      print(pred.argmax(dim=1).float()[0:5])\n",
        "      print(data.y[0:5])\n",
        "      print(\"train acc: \"+str(train_accuracies[-1]))\n",
        "      print(\"test acc: \"+str(test_accuracies[-1]))\n",
        "      print(\"val acc: \"+str(val_accuracies[-1]))\n",
        "      print(\"train loss: \"+str(losses[-1]))\n",
        "      print(\"===================================\")\n",
        "      '''\n",
        "      if epoch!=0:\n",
        "        if (val_accuracies[-1]>val_accuracies[-11]):\n",
        "          streaks += 1\n",
        "        else:\n",
        "          streaks = 0\n",
        "      '''\n",
        "    if streaks >= max_patience and train_accuracies[-1]>0.7:\n",
        "      print(\"===============Epoch\"+str(epoch)+\"==============\")\n",
        "      print(\"train acc: \"+str(train_accuracies[-1]))\n",
        "      print(\"test acc: \"+str(test_accuracies[-1]))\n",
        "      print(\"val acc: \"+str(val_accuracies[-1]))\n",
        "      print(\"train loss: \"+str(losses[-1]))\n",
        "      print(\"===================================\")\n",
        "      files.download(\"X.pickle\")\n",
        "      break\n",
        "  print(\"train acc: \"+str(max(train_accuracies)))\n",
        "  print(\"test acc: \"+str(max(test_accuracies)))\n",
        "  print(\"val acc: \"+str(max(val_accuracies)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8zMHgekSAs-h",
        "outputId": "2b5d5dda-a1da-43d0-90eb-be2658330899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============Epoch0==============\n",
            "tensor([[ 0.7464,  0.7241,  0.7359, -0.1351, -0.8409, -0.8902, -1.1736, -1.0024,\n",
            "         -0.3276],\n",
            "        [ 0.7706,  0.7597,  0.7724, -0.1320, -0.8825, -0.9257, -1.2252, -1.0485,\n",
            "         -0.3258],\n",
            "        [ 0.7464,  0.7241,  0.7359, -0.1351, -0.8409, -0.8902, -1.1736, -1.0024,\n",
            "         -0.3276],\n",
            "        [ 0.7463,  0.7247,  0.7364, -0.1353, -0.8416, -0.8903, -1.1737, -1.0029,\n",
            "         -0.3272],\n",
            "        [ 0.8333,  0.8348,  0.8512, -0.1203, -0.9703, -1.0109, -1.3481, -1.1515,\n",
            "         -0.3319]], grad_fn=<SliceBackward0>)\n",
            "tensor([0., 2., 0., 0., 2.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.4668)\n",
            "test acc: tensor(0.4681)\n",
            "val acc: tensor(0.4611)\n",
            "train loss: 1.9383614796858568\n",
            "===================================\n",
            "===============Epoch10==============\n",
            "tensor([[ 2.2865,  1.4835,  1.4877,  0.6114,  0.0947, -1.2333, -2.1491, -3.7824,\n",
            "         -3.5483],\n",
            "        [ 2.4174,  1.5849,  1.5558,  0.6579,  0.0799, -1.3012, -2.2796, -4.0200,\n",
            "         -3.7374],\n",
            "        [ 2.2854,  1.4822,  1.4868,  0.6112,  0.0953, -1.2326, -2.1478, -3.7799,\n",
            "         -3.5469],\n",
            "        [ 2.2991,  1.4994,  1.4984,  0.6139,  0.0876, -1.2408, -2.1638, -3.8121,\n",
            "         -3.5647],\n",
            "        [ 2.6790,  1.7576,  1.6717,  0.7603,  0.0779, -1.4319, -2.5296, -4.4613,\n",
            "         -4.1240]], grad_fn=<SliceBackward0>)\n",
            "tensor([0., 0., 0., 0., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.4672)\n",
            "test acc: tensor(0.4681)\n",
            "val acc: tensor(0.4614)\n",
            "train loss: 1.4237658702410185\n",
            "===================================\n",
            "===============Epoch20==============\n",
            "tensor([[ 2.2324,  1.5198,  1.6308,  0.7687,  0.3072, -1.0388, -1.9739, -3.6170,\n",
            "         -4.0024],\n",
            "        [ 2.7135,  1.8587,  1.7246,  0.8831,  0.1715, -1.2302, -2.3710, -4.3698,\n",
            "         -4.6494],\n",
            "        [ 2.2324,  1.5198,  1.6308,  0.7687,  0.3072, -1.0388, -1.9739, -3.6170,\n",
            "         -4.0024],\n",
            "        [ 2.2526,  1.5347,  1.6345,  0.7723,  0.2996, -1.0469, -1.9905, -3.6489,\n",
            "         -4.0278],\n",
            "        [ 3.3353,  2.2768,  1.8518,  1.0667,  0.0510, -1.4744, -2.8854, -5.3339,\n",
            "         -5.5348]], grad_fn=<SliceBackward0>)\n",
            "tensor([0., 0., 0., 0., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.4683)\n",
            "test acc: tensor(0.4764)\n",
            "val acc: tensor(0.4617)\n",
            "train loss: 1.3963031768798828\n",
            "===================================\n",
            "===============Epoch30==============\n",
            "tensor([[ 2.0713,  1.5627,  1.7473,  0.8883,  0.4820, -0.9410, -1.8919, -3.5577,\n",
            "         -4.5397],\n",
            "        [ 2.7794,  1.9456,  1.7997,  0.9837,  0.2317, -1.2067, -2.4219, -4.4733,\n",
            "         -5.4396],\n",
            "        [ 2.0713,  1.5627,  1.7473,  0.8883,  0.4820, -0.9410, -1.8919, -3.5577,\n",
            "         -4.5397],\n",
            "        [ 2.0927,  1.5736,  1.7474,  0.8887,  0.4717, -0.9492, -1.9072, -3.5835,\n",
            "         -4.5627],\n",
            "        [ 3.5076,  2.3693,  1.9229,  1.2029,  0.0996, -1.4739, -3.0025, -5.5069,\n",
            "         -6.5640]], grad_fn=<SliceBackward0>)\n",
            "tensor([0., 0., 0., 0., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.4938)\n",
            "test acc: tensor(0.5008)\n",
            "val acc: tensor(0.4837)\n",
            "train loss: 1.3717406667195833\n",
            "===================================\n",
            "===============Epoch40==============\n",
            "tensor([[ 1.4479,  1.8113,  2.2512,  1.4596,  1.1925, -0.3468, -1.3291, -3.3045,\n",
            "         -5.5127],\n",
            "        [ 2.9667,  2.1812,  1.9638,  1.1038,  0.2861, -1.0737, -2.2337, -4.3341,\n",
            "         -6.6075],\n",
            "        [ 1.4479,  1.8113,  2.2513,  1.4596,  1.1925, -0.3468, -1.3291, -3.3045,\n",
            "         -5.5127],\n",
            "        [ 1.4536,  1.8121,  2.2496,  1.4574,  1.1884, -0.3498, -1.3323, -3.3075,\n",
            "         -5.5151],\n",
            "        [ 3.5723,  2.6377,  2.1755,  1.4523,  0.3204, -1.2364, -2.6912, -5.2199,\n",
            "         -7.9834]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5188)\n",
            "test acc: tensor(0.5188)\n",
            "val acc: tensor(0.5145)\n",
            "train loss: 1.2619024927799518\n",
            "===================================\n",
            "===============Epoch50==============\n",
            "tensor([[ 1.2605,  1.9689,  2.4662,  1.7745,  1.5919,  0.1701, -0.8292, -2.8361,\n",
            "         -5.6826],\n",
            "        [ 3.1763,  2.3948,  2.1207,  1.1878,  0.1933, -1.2093, -2.3039, -3.9070,\n",
            "         -6.7412],\n",
            "        [ 1.2605,  1.9689,  2.4662,  1.7745,  1.5919,  0.1701, -0.8292, -2.8361,\n",
            "         -5.6826],\n",
            "        [ 1.2605,  1.9689,  2.4662,  1.7745,  1.5919,  0.1701, -0.8292, -2.8361,\n",
            "         -5.6826],\n",
            "        [ 3.7726,  2.8626,  2.3332,  1.5671,  0.2291, -1.3517, -2.7252, -4.6304,\n",
            "         -8.0717]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5151)\n",
            "test acc: tensor(0.5157)\n",
            "val acc: tensor(0.5105)\n",
            "train loss: 1.2393747843228853\n",
            "===================================\n",
            "===============Epoch60==============\n",
            "tensor([[ 1.2428,  2.0116,  2.5368,  1.9025,  1.7737,  0.4417, -0.5486, -2.6013,\n",
            "         -5.3272],\n",
            "        [ 3.2802,  2.5070,  2.2269,  1.2576,  0.1879, -1.3886, -2.5218, -3.7794,\n",
            "         -6.3070],\n",
            "        [ 1.2428,  2.0116,  2.5368,  1.9025,  1.7737,  0.4417, -0.5486, -2.6013,\n",
            "         -5.3272],\n",
            "        [ 1.2428,  2.0116,  2.5368,  1.9025,  1.7737,  0.4417, -0.5486, -2.6013,\n",
            "         -5.3272],\n",
            "        [ 3.8626,  2.9572,  2.4243,  1.6609,  0.2585, -1.5065, -2.9346, -4.4233,\n",
            "         -7.4960]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5215)\n",
            "test acc: tensor(0.5243)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2332724286959722\n",
            "===================================\n",
            "===============Epoch70==============\n",
            "tensor([[ 1.2589,  2.0359,  2.5653,  1.9583,  1.8456,  0.5674, -0.4152, -2.4508,\n",
            "         -4.8232],\n",
            "        [ 3.3536,  2.5747,  2.2850,  1.2928,  0.1862, -1.5483, -2.7860, -3.7669,\n",
            "         -5.7369],\n",
            "        [ 1.2589,  2.0359,  2.5653,  1.9583,  1.8456,  0.5674, -0.4152, -2.4508,\n",
            "         -4.8232],\n",
            "        [ 1.2589,  2.0359,  2.5653,  1.9583,  1.8456,  0.5674, -0.4152, -2.4508,\n",
            "         -4.8232],\n",
            "        [ 3.9255,  3.0055,  2.4621,  1.7068,  0.2887, -1.6310, -3.1967, -4.3633,\n",
            "         -6.7780]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5193)\n",
            "test acc: tensor(0.5236)\n",
            "val acc: tensor(0.5175)\n",
            "train loss: 1.2305233065898602\n",
            "===================================\n",
            "===============Epoch80==============\n",
            "tensor([[ 1.2607,  2.0505,  2.5834,  1.9920,  1.8880,  0.6411, -0.3407, -2.3308,\n",
            "         -4.4311],\n",
            "        [ 3.3974,  2.6180,  2.3223,  1.3143,  0.1890, -1.6645, -3.0280, -3.8056,\n",
            "         -5.3240],\n",
            "        [ 1.2607,  2.0505,  2.5834,  1.9920,  1.8880,  0.6411, -0.3407, -2.3308,\n",
            "         -4.4311],\n",
            "        [ 1.2607,  2.0505,  2.5834,  1.9920,  1.8880,  0.6411, -0.3407, -2.3308,\n",
            "         -4.4311],\n",
            "        [ 3.9599,  3.0328,  2.4810,  1.7315,  0.3152, -1.7038, -3.4326, -4.3692,\n",
            "         -6.2611]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5193)\n",
            "test acc: tensor(0.5236)\n",
            "val acc: tensor(0.5175)\n",
            "train loss: 1.2290276334835932\n",
            "===================================\n",
            "===============Epoch90==============\n",
            "tensor([[ 1.2591,  2.0599,  2.5926,  2.0114,  1.9054,  0.6773, -0.3026, -2.2359,\n",
            "         -4.2192],\n",
            "        [ 3.4231,  2.6482,  2.3465,  1.3304,  0.1850, -1.7547, -3.2309, -3.8779,\n",
            "         -5.1382],\n",
            "        [ 1.2591,  2.0599,  2.5926,  2.0114,  1.9054,  0.6773, -0.3026, -2.2359,\n",
            "         -4.2192],\n",
            "        [ 1.2591,  2.0599,  2.5926,  2.0114,  1.9054,  0.6773, -0.3026, -2.2359,\n",
            "         -4.2192],\n",
            "        [ 3.9768,  3.0500,  2.4895,  1.7466,  0.3257, -1.7482, -3.6224, -4.4168,\n",
            "         -6.0213]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2281020008600676\n",
            "===================================\n",
            "===============Epoch100==============\n",
            "tensor([[ 1.2506,  2.0610,  2.5956,  2.0198,  1.9177,  0.7137, -0.2711, -2.1614,\n",
            "         -4.1106],\n",
            "        [ 3.4354,  2.6640,  2.3603,  1.3374,  0.1824, -1.8090, -3.3844, -3.9748,\n",
            "         -5.0843],\n",
            "        [ 1.2506,  2.0610,  2.5956,  2.0198,  1.9177,  0.7137, -0.2711, -2.1614,\n",
            "         -4.1106],\n",
            "        [ 1.2506,  2.0610,  2.5956,  2.0198,  1.9177,  0.7137, -0.2711, -2.1614,\n",
            "         -4.1106],\n",
            "        [ 3.9816,  3.0544,  2.4898,  1.7481,  0.3301, -1.7562, -3.7568, -4.4952,\n",
            "         -5.9411]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2275197826899016\n",
            "===================================\n",
            "===============Epoch110==============\n",
            "tensor([[ 1.2388,  2.0515,  2.5876,  2.0119,  1.9165,  0.7357, -0.2514, -2.1052,\n",
            "         -4.0356],\n",
            "        [ 3.4391,  2.6657,  2.3607,  1.3310,  0.1707, -1.8575, -3.5089, -4.0907,\n",
            "         -5.0827],\n",
            "        [ 1.2388,  2.0515,  2.5876,  2.0119,  1.9165,  0.7357, -0.2514, -2.1052,\n",
            "         -4.0356],\n",
            "        [ 1.2388,  2.0515,  2.5876,  2.0119,  1.9165,  0.7357, -0.2514, -2.1052,\n",
            "         -4.0356],\n",
            "        [ 3.9776,  3.0453,  2.4777,  1.7333,  0.3201, -1.7608, -3.8573, -4.5955,\n",
            "         -5.9243]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2271292072076063\n",
            "===================================\n",
            "===============Epoch120==============\n",
            "tensor([[ 1.2173,  2.0383,  2.5785,  2.0058,  1.9134,  0.7506, -0.2354, -2.0544,\n",
            "         -3.9732],\n",
            "        [ 3.4288,  2.6610,  2.3583,  1.3289,  0.1599, -1.9026, -3.6081, -4.2070,\n",
            "         -5.1066],\n",
            "        [ 1.2173,  2.0383,  2.5785,  2.0058,  1.9134,  0.7506, -0.2354, -2.0544,\n",
            "         -3.9732],\n",
            "        [ 1.2173,  2.0383,  2.5785,  2.0058,  1.9134,  0.7506, -0.2354, -2.0544,\n",
            "         -3.9732],\n",
            "        [ 3.9588,  3.0307,  2.4646,  1.7230,  0.3076, -1.7661, -3.9300, -4.6962,\n",
            "         -5.9380]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2267717856627245\n",
            "===================================\n",
            "===============Epoch130==============\n",
            "tensor([[ 1.1999,  2.0239,  2.5651,  1.9916,  1.9006,  0.7536, -0.2334, -2.0239,\n",
            "         -3.9244],\n",
            "        [ 3.4193,  2.6535,  2.3507,  1.3202,  0.1423, -1.9512, -3.6991, -4.3369,\n",
            "         -5.1565],\n",
            "        [ 1.1999,  2.0239,  2.5651,  1.9916,  1.9006,  0.7536, -0.2334, -2.0239,\n",
            "         -3.9244],\n",
            "        [ 1.1999,  2.0239,  2.5651,  1.9916,  1.9006,  0.7536, -0.2334, -2.0239,\n",
            "         -3.9244],\n",
            "        [ 3.9413,  3.0144,  2.4473,  1.7051,  0.2856, -1.7785, -3.9935, -4.8118,\n",
            "         -5.9820]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2264849543571472\n",
            "===================================\n",
            "===============Epoch140==============\n",
            "tensor([[ 1.1851,  2.0079,  2.5485,  1.9722,  1.8840,  0.7490, -0.2449, -2.0114,\n",
            "         -3.8850],\n",
            "        [ 3.4087,  2.6435,  2.3397,  1.3081,  0.1240, -2.0019, -3.7897, -4.4775,\n",
            "         -5.2285],\n",
            "        [ 1.1851,  2.0079,  2.5485,  1.9722,  1.8840,  0.7490, -0.2449, -2.0114,\n",
            "         -3.8850],\n",
            "        [ 1.1851,  2.0079,  2.5485,  1.9722,  1.8840,  0.7490, -0.2449, -2.0114,\n",
            "         -3.8850],\n",
            "        [ 3.9218,  2.9960,  2.4278,  1.6844,  0.2631, -1.7941, -4.0550, -4.9374,\n",
            "         -6.0509]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2262754211058984\n",
            "===================================\n",
            "===============Epoch150==============\n",
            "tensor([[ 1.1645,  1.9896,  2.5314,  1.9542,  1.8683,  0.7471, -0.2462, -1.9960,\n",
            "         -3.8446],\n",
            "        [ 3.3943,  2.6308,  2.3274,  1.2963,  0.1042, -2.0526, -3.8658, -4.6094,\n",
            "         -5.3122],\n",
            "        [ 1.1645,  1.9896,  2.5314,  1.9542,  1.8683,  0.7471, -0.2462, -1.9960,\n",
            "         -3.8446],\n",
            "        [ 1.1645,  1.9896,  2.5314,  1.9542,  1.8683,  0.7471, -0.2462, -1.9960,\n",
            "         -3.8446],\n",
            "        [ 3.9001,  2.9757,  2.4072,  1.6635,  0.2363, -1.8150, -4.1036, -5.0532,\n",
            "         -6.1332]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2260734347196727\n",
            "===================================\n",
            "===============Epoch160==============\n",
            "tensor([[ 1.1427,  1.9715,  2.5146,  1.9358,  1.8521,  0.7443, -0.2473, -1.9830,\n",
            "         -3.8044],\n",
            "        [ 3.3774,  2.6172,  2.3140,  1.2841,  0.0860, -2.0983, -3.9297, -4.7300,\n",
            "         -5.4061],\n",
            "        [ 1.1427,  1.9715,  2.5146,  1.9358,  1.8521,  0.7443, -0.2473, -1.9830,\n",
            "         -3.8044],\n",
            "        [ 1.1427,  1.9715,  2.5146,  1.9358,  1.8521,  0.7443, -0.2473, -1.9830,\n",
            "         -3.8044],\n",
            "        [ 3.8760,  2.9550,  2.3864,  1.6425,  0.2106, -1.8328, -4.1394, -5.1547,\n",
            "         -6.2267]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2258334801747248\n",
            "===================================\n",
            "===============Epoch170==============\n",
            "tensor([[ 1.1236,  1.9533,  2.4967,  1.9162,  1.8339,  0.7378, -0.2527, -1.9764,\n",
            "         -3.7701],\n",
            "        [ 3.3620,  2.6033,  2.2998,  1.2711,  0.0665, -2.1463, -3.9937, -4.8472,\n",
            "         -5.5182],\n",
            "        [ 1.1236,  1.9533,  2.4967,  1.9162,  1.8339,  0.7378, -0.2527, -1.9764,\n",
            "         -3.7701],\n",
            "        [ 1.1236,  1.9533,  2.4967,  1.9162,  1.8339,  0.7378, -0.2527, -1.9764,\n",
            "         -3.7701],\n",
            "        [ 3.8542,  2.9344,  2.3651,  1.6210,  0.1833, -1.8554, -4.1765, -5.2511,\n",
            "         -6.3404]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5204)\n",
            "test acc: tensor(0.5246)\n",
            "val acc: tensor(0.5188)\n",
            "train loss: 1.2256425206477826\n",
            "===================================\n",
            "===============Epoch180==============\n",
            "tensor([[ 1.1059,  1.9362,  2.4828,  1.9017,  1.8254,  0.7466, -0.2433, -1.9677,\n",
            "         -3.7451],\n",
            "        [ 3.3431,  2.5883,  2.2899,  1.2641,  0.0622, -2.1693, -4.0285, -4.9448,\n",
            "         -5.6471],\n",
            "        [ 1.1059,  1.9362,  2.4828,  1.9017,  1.8254,  0.7466, -0.2433, -1.9677,\n",
            "         -3.7451],\n",
            "        [ 1.1059,  1.9362,  2.4828,  1.9017,  1.8254,  0.7466, -0.2433, -1.9677,\n",
            "         -3.7451],\n",
            "        [ 3.8271,  2.9127,  2.3495,  1.6077,  0.1749, -1.8481, -4.1780, -5.3213,\n",
            "         -6.4715]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.225515448130094\n",
            "===================================\n",
            "===============Epoch190==============\n",
            "tensor([[ 1.0741,  1.9143,  2.4628,  1.8822,  1.8029,  0.7336, -0.2523, -1.9533,\n",
            "         -3.6940],\n",
            "        [ 3.3263,  2.5732,  2.2710,  1.2461,  0.0267, -2.2402, -4.1134, -5.0508,\n",
            "         -5.7732],\n",
            "        [ 1.0741,  1.9143,  2.4628,  1.8822,  1.8029,  0.7336, -0.2523, -1.9533,\n",
            "         -3.6940],\n",
            "        [ 1.0741,  1.9143,  2.4628,  1.8822,  1.8029,  0.7336, -0.2523, -1.9533,\n",
            "         -3.6940],\n",
            "        [ 3.8080,  2.8923,  2.3231,  1.5795,  0.1251, -1.9071, -4.2492, -5.4082,\n",
            "         -6.6021]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.2252992666684663\n",
            "===================================\n",
            "===============Epoch200==============\n",
            "tensor([[ 1.0421,  1.8936,  2.4474,  1.8701,  1.7988,  0.7499, -0.2288, -1.9347,\n",
            "         -3.6641],\n",
            "        [ 3.2937,  2.5556,  2.2595,  1.2416,  0.0254, -2.2554, -4.1334, -5.1239,\n",
            "         -5.9232],\n",
            "        [ 1.0421,  1.8936,  2.4474,  1.8701,  1.7988,  0.7499, -0.2288, -1.9347,\n",
            "         -3.6641],\n",
            "        [ 1.0421,  1.8936,  2.4474,  1.8701,  1.7988,  0.7499, -0.2288, -1.9347,\n",
            "         -3.6641],\n",
            "        [ 3.7653,  2.8683,  2.3064,  1.5701,  0.1207, -1.8932, -4.2350, -5.4474,\n",
            "         -6.7552]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.2251383112027094\n",
            "===================================\n",
            "===============Epoch210==============\n",
            "tensor([[ 1.0342,  1.8768,  2.4295,  1.8474,  1.7805,  0.7360, -0.2454, -1.9460,\n",
            "         -3.6561],\n",
            "        [ 3.2842,  2.5405,  2.2449,  1.2259,  0.0124, -2.2955, -4.1872, -5.2149,\n",
            "         -6.1024],\n",
            "        [ 1.0342,  1.8768,  2.4295,  1.8474,  1.7805,  0.7360, -0.2454, -1.9460,\n",
            "         -3.6561],\n",
            "        [ 1.0342,  1.8768,  2.4295,  1.8474,  1.7805,  0.7360, -0.2454, -1.9460,\n",
            "         -3.6561],\n",
            "        [ 3.7497,  2.8474,  2.2865,  1.5480,  0.1027, -1.9098, -4.2612, -5.5067,\n",
            "         -6.9409]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.2250357637038598\n",
            "===================================\n",
            "===============Epoch220==============\n",
            "tensor([[ 1.0201,  1.8608,  2.4104,  1.8252,  1.7511,  0.7085, -0.2752, -1.9569,\n",
            "         -3.6411],\n",
            "        [ 3.2797,  2.5297,  2.2275,  1.2063, -0.0247, -2.3694, -4.2788, -5.3178,\n",
            "         -6.2959],\n",
            "        [ 1.0201,  1.8608,  2.4104,  1.8252,  1.7511,  0.7085, -0.2752, -1.9569,\n",
            "         -3.6411],\n",
            "        [ 1.0201,  1.8608,  2.4104,  1.8252,  1.7511,  0.7085, -0.2752, -1.9569,\n",
            "         -3.6411],\n",
            "        [ 3.7441,  2.8325,  2.2630,  1.5197,  0.0529, -1.9738, -4.3406, -5.5859,\n",
            "         -7.1457]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.2249517670044532\n",
            "===================================\n",
            "===============Epoch230==============\n",
            "tensor([[ 0.9955,  1.8416,  2.3966,  1.8131,  1.7493,  0.7235, -0.2570, -1.9406,\n",
            "         -3.6281],\n",
            "        [ 3.2521,  2.5116,  2.2169,  1.2010, -0.0202, -2.3759, -4.2917, -5.3654,\n",
            "         -6.4865],\n",
            "        [ 0.9955,  1.8416,  2.3966,  1.8131,  1.7493,  0.7235, -0.2570, -1.9406,\n",
            "         -3.6281],\n",
            "        [ 0.9955,  1.8416,  2.3966,  1.8131,  1.7493,  0.7235, -0.2570, -1.9406,\n",
            "         -3.6281],\n",
            "        [ 3.7078,  2.8085,  2.2481,  1.5104,  0.0555, -1.9534, -4.3210, -5.5935,\n",
            "         -7.3416]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.224825313458076\n",
            "===================================\n",
            "===============Epoch240==============\n",
            "tensor([[ 0.9760,  1.8243,  2.3805,  1.7965,  1.7349,  0.7175, -0.2604, -1.9376,\n",
            "         -3.6255],\n",
            "        [ 3.2358,  2.4973,  2.2034,  1.1886, -0.0354, -2.4139, -4.3396, -5.4327,\n",
            "         -6.7032],\n",
            "        [ 0.9760,  1.8243,  2.3805,  1.7965,  1.7349,  0.7175, -0.2604, -1.9376,\n",
            "         -3.6255],\n",
            "        [ 0.9760,  1.8243,  2.3805,  1.7965,  1.7349,  0.7175, -0.2604, -1.9376,\n",
            "         -3.6255],\n",
            "        [ 3.6867,  2.7896,  2.2299,  1.4922,  0.0335, -1.9744, -4.3471, -5.6271,\n",
            "         -7.5687]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5198)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5151)\n",
            "train loss: 1.2247365438021147\n",
            "===================================\n",
            "===============Epoch250==============\n",
            "tensor([[ 0.9627,  1.8103,  2.3636,  1.7774,  1.7073,  0.6919, -0.2803, -1.9435,\n",
            "         -3.6269],\n",
            "        [ 3.2295,  2.4870,  2.1875,  1.1718, -0.0682, -2.4761, -4.4078, -5.5081,\n",
            "         -6.9316],\n",
            "        [ 0.9627,  1.8103,  2.3636,  1.7774,  1.7073,  0.6919, -0.2803, -1.9435,\n",
            "         -3.6269],\n",
            "        [ 0.9627,  1.8103,  2.3636,  1.7774,  1.7073,  0.6919, -0.2803, -1.9435,\n",
            "         -3.6269],\n",
            "        [ 3.6797,  2.7759,  2.2088,  1.4678, -0.0111, -2.0289, -4.4030, -5.6744,\n",
            "         -7.8110]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5187)\n",
            "test acc: tensor(0.5215)\n",
            "val acc: tensor(0.5139)\n",
            "train loss: 1.2246039097125714\n",
            "===================================\n",
            "===============Epoch260==============\n",
            "tensor([[ 0.9330,  1.7920,  2.3509,  1.7683,  1.7045,  0.7052, -0.2680, -1.9257,\n",
            "         -3.6237],\n",
            "        [ 3.2027,  2.4717,  2.1770,  1.1665, -0.0721, -2.4928, -4.4381, -5.5534,\n",
            "         -7.1581],\n",
            "        [ 0.9330,  1.7920,  2.3509,  1.7683,  1.7045,  0.7052, -0.2680, -1.9257,\n",
            "         -3.6237],\n",
            "        [ 0.9330,  1.7920,  2.3509,  1.7683,  1.7045,  0.7052, -0.2680, -1.9257,\n",
            "         -3.6237],\n",
            "        [ 3.6461,  2.7560,  2.1944,  1.4585, -0.0190, -2.0265, -4.4098, -5.6820,\n",
            "         -8.0464]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5187)\n",
            "test acc: tensor(0.5215)\n",
            "val acc: tensor(0.5139)\n",
            "train loss: 1.224532452913431\n",
            "===================================\n",
            "===============Epoch270==============\n",
            "tensor([[ 0.9121,  1.7751,  2.3364,  1.7538,  1.6934,  0.7040, -0.2650, -1.9170,\n",
            "         -3.6315],\n",
            "        [ 3.1849,  2.4571,  2.1641,  1.1552, -0.0839, -2.5213, -4.4736, -5.6023,\n",
            "         -7.3962],\n",
            "        [ 0.9121,  1.7751,  2.3364,  1.7538,  1.6934,  0.7040, -0.2650, -1.9170,\n",
            "         -3.6315],\n",
            "        [ 0.9121,  1.7751,  2.3364,  1.7538,  1.6934,  0.7040, -0.2650, -1.9170,\n",
            "         -3.6315],\n",
            "        [ 3.6239,  2.7372,  2.1772,  1.4420, -0.0369, -2.0405, -4.4253, -5.6955,\n",
            "         -8.2953]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5187)\n",
            "test acc: tensor(0.5215)\n",
            "val acc: tensor(0.5139)\n",
            "train loss: 1.2244553245030916\n",
            "===================================\n",
            "===============Epoch280==============\n",
            "tensor([[ 0.9086,  1.7611,  2.3211,  1.7342,  1.6759,  0.6862, -0.2850, -1.9337,\n",
            "         -3.6785],\n",
            "        [ 3.1780,  2.4434,  2.1514,  1.1408, -0.0954, -2.5531, -4.5165, -5.6628,\n",
            "         -7.6646],\n",
            "        [ 0.9086,  1.7611,  2.3211,  1.7342,  1.6759,  0.6862, -0.2850, -1.9337,\n",
            "         -3.6785],\n",
            "        [ 0.9086,  1.7611,  2.3211,  1.7342,  1.6759,  0.6862, -0.2850, -1.9337,\n",
            "         -3.6785],\n",
            "        [ 3.6122,  2.7190,  2.1607,  1.4232, -0.0521, -2.0550, -4.4453, -5.7186,\n",
            "         -8.5742]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2244342657235951\n",
            "===================================\n",
            "===============Epoch290==============\n",
            "tensor([[ 0.8837,  1.7459,  2.3082,  1.7238,  1.6642,  0.6879, -0.2786, -1.9207,\n",
            "         -3.6895],\n",
            "        [ 3.1577,  2.4311,  2.1394,  1.1325, -0.1108, -2.5798, -4.5496, -5.7057,\n",
            "         -7.9051],\n",
            "        [ 0.8837,  1.7459,  2.3082,  1.7238,  1.6642,  0.6879, -0.2786, -1.9207,\n",
            "         -3.6895],\n",
            "        [ 0.8837,  1.7459,  2.3082,  1.7238,  1.6642,  0.6879, -0.2786, -1.9207,\n",
            "         -3.6895],\n",
            "        [ 3.5881,  2.7032,  2.1447,  1.4102, -0.0745, -2.0706, -4.4619, -5.7276,\n",
            "         -8.8248]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5187)\n",
            "test acc: tensor(0.5215)\n",
            "val acc: tensor(0.5139)\n",
            "train loss: 1.22432384124169\n",
            "===================================\n",
            "===============Epoch300==============\n",
            "tensor([[ 0.8759,  1.7333,  2.2954,  1.7078,  1.6546,  0.6834, -0.2799, -1.9221,\n",
            "         -3.7377],\n",
            "        [ 3.1472,  2.4186,  2.1287,  1.1199, -0.1154, -2.5978, -4.5717, -5.7468,\n",
            "         -8.1658],\n",
            "        [ 0.8759,  1.7333,  2.2954,  1.7078,  1.6546,  0.6834, -0.2799, -1.9221,\n",
            "         -3.7377],\n",
            "        [ 0.8759,  1.7333,  2.2954,  1.7078,  1.6546,  0.6834, -0.2799, -1.9221,\n",
            "         -3.7377],\n",
            "        [ 3.5735,  2.6868,  2.1306,  1.3933, -0.0826, -2.0733, -4.4622, -5.7313,\n",
            "         -9.0942]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2242854971152086\n",
            "===================================\n",
            "===============Epoch310==============\n",
            "tensor([[ 0.8549,  1.7171,  2.2804,  1.6924,  1.6376,  0.6666, -0.2942, -1.9227,\n",
            "         -3.7570],\n",
            "        [ 3.1359,  2.4065,  2.1139,  1.1043, -0.1411, -2.6508, -4.6365, -5.8109,\n",
            "         -8.4152],\n",
            "        [ 0.8549,  1.7171,  2.2804,  1.6924,  1.6376,  0.6666, -0.2942, -1.9227,\n",
            "         -3.7570],\n",
            "        [ 0.8549,  1.7171,  2.2804,  1.6924,  1.6376,  0.6666, -0.2942, -1.9227,\n",
            "         -3.7570],\n",
            "        [ 3.5611,  2.6716,  2.1115,  1.3716, -0.1177, -2.1239, -4.5203, -5.7687,\n",
            "         -9.3560]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5142)\n",
            "test acc: tensor(0.5200)\n",
            "val acc: tensor(0.5124)\n",
            "train loss: 1.2242506375679603\n",
            "===================================\n",
            "===============Epoch320==============\n",
            "tensor([[ 0.8425,  1.7040,  2.2679,  1.6795,  1.6256,  0.6599, -0.3016, -1.9261,\n",
            "         -3.8027],\n",
            "        [ 3.1230,  2.3944,  2.1030,  1.0946, -0.1513, -2.6737, -4.6699, -5.8573,\n",
            "         -8.6644],\n",
            "        [ 0.8425,  1.7040,  2.2679,  1.6795,  1.6256,  0.6599, -0.3016, -1.9261,\n",
            "         -3.8027],\n",
            "        [ 0.8425,  1.7040,  2.2679,  1.6795,  1.6256,  0.6599, -0.3016, -1.9261,\n",
            "         -3.8027],\n",
            "        [ 3.5439,  2.6559,  2.0973,  1.3582, -0.1319, -2.1344, -4.5358, -5.7799,\n",
            "         -9.6108]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2241849486644452\n",
            "===================================\n",
            "===============Epoch330==============\n",
            "tensor([[ 0.8342,  1.6930,  2.2575,  1.6681,  1.6143,  0.6524, -0.3083, -1.9287,\n",
            "         -3.8519],\n",
            "        [ 3.1121,  2.3830,  2.0938,  1.0861, -0.1586, -2.6916, -4.6933, -5.8941,\n",
            "         -8.8965],\n",
            "        [ 0.8342,  1.6930,  2.2575,  1.6681,  1.6143,  0.6524, -0.3083, -1.9287,\n",
            "         -3.8519],\n",
            "        [ 0.8342,  1.6930,  2.2575,  1.6681,  1.6143,  0.6524, -0.3083, -1.9287,\n",
            "         -3.8519],\n",
            "        [ 3.5292,  2.6410,  2.0850,  1.3462, -0.1429, -2.1409, -4.5414, -5.7813,\n",
            "         -9.8455]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2241818308830261\n",
            "===================================\n",
            "===============Epoch340==============\n",
            "tensor([[  0.8142,   1.6781,   2.2460,   1.6582,   1.6057,   0.6499,  -0.3062,\n",
            "          -1.9212,  -3.8757],\n",
            "        [  3.0945,   2.3693,   2.0826,   1.0776,  -0.1691,  -2.7133,  -4.7193,\n",
            "          -5.9306,  -9.1041],\n",
            "        [  0.8142,   1.6781,   2.2460,   1.6582,   1.6057,   0.6499,  -0.3062,\n",
            "          -1.9212,  -3.8757],\n",
            "        [  0.8142,   1.6781,   2.2460,   1.6582,   1.6057,   0.6499,  -0.3062,\n",
            "          -1.9212,  -3.8757],\n",
            "        [  3.5075,   2.6238,   2.0706,   1.3341,  -0.1580,  -2.1539,  -4.5525,\n",
            "          -5.7845, -10.0528]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2241276777707613\n",
            "===================================\n",
            "===============Epoch350==============\n",
            "tensor([[  0.8013,   1.6657,   2.2344,   1.6462,   1.5943,   0.6421,  -0.3125,\n",
            "          -1.9225,  -3.9150],\n",
            "        [  3.0822,   2.3579,   2.0718,   1.0674,  -0.1807,  -2.7368,  -4.7501,\n",
            "          -5.9725,  -9.3132],\n",
            "        [  0.8013,   1.6657,   2.2344,   1.6462,   1.5943,   0.6421,  -0.3125,\n",
            "          -1.9225,  -3.9150],\n",
            "        [  0.8013,   1.6657,   2.2344,   1.6462,   1.5943,   0.6422,  -0.3125,\n",
            "          -1.9225,  -3.9150],\n",
            "        [  3.4920,   2.6092,   2.0567,   1.3201,  -0.1741,  -2.1695,  -4.5698,\n",
            "          -5.7944, -10.2616]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2240973160817072\n",
            "===================================\n",
            "===============Epoch360==============\n",
            "tensor([[  0.7891,   1.6537,   2.2229,   1.6339,   1.5821,   0.6343,  -0.3169,\n",
            "          -1.9235,  -3.9522],\n",
            "        [  3.0706,   2.3466,   2.0611,   1.0566,  -0.1930,  -2.7585,  -4.7765,\n",
            "          -6.0120,  -9.5070],\n",
            "        [  0.7891,   1.6537,   2.2229,   1.6339,   1.5821,   0.6343,  -0.3169,\n",
            "          -1.9235,  -3.9522],\n",
            "        [  0.7891,   1.6537,   2.2229,   1.6339,   1.5821,   0.6343,  -0.3169,\n",
            "          -1.9235,  -3.9522],\n",
            "        [  3.4773,   2.5947,   2.0427,   1.3056,  -0.1911,  -2.1843,  -4.5830,\n",
            "          -5.8025, -10.4527]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2240666976341834\n",
            "===================================\n",
            "===============Epoch370==============\n",
            "tensor([[  0.7731,   1.6429,   2.2119,   1.6241,   1.5704,   0.6273,  -0.3264,\n",
            "          -1.9262,  -3.9836],\n",
            "        [  3.0565,   2.3374,   2.0504,   1.0480,  -0.2070,  -2.7824,  -4.8131,\n",
            "          -6.0574,  -9.6881],\n",
            "        [  0.7731,   1.6429,   2.2119,   1.6241,   1.5704,   0.6273,  -0.3264,\n",
            "          -1.9262,  -3.9836],\n",
            "        [  0.7731,   1.6429,   2.2119,   1.6241,   1.5704,   0.6273,  -0.3264,\n",
            "          -1.9262,  -3.9836],\n",
            "        [  3.4598,   2.5827,   2.0291,   1.2936,  -0.2097,  -2.2023,  -4.6089,\n",
            "          -5.8181, -10.6292]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2240318151620717\n",
            "===================================\n",
            "===============Epoch380==============\n",
            "tensor([[  0.7641,   1.6306,   2.2016,   1.6123,   1.5618,   0.6190,  -0.3316,\n",
            "          -1.9285,  -4.0191],\n",
            "        [  3.0468,   2.3248,   2.0407,   1.0375,  -0.2140,  -2.8008,  -4.8350,\n",
            "          -6.0935,  -9.8515],\n",
            "        [  0.7641,   1.6306,   2.2016,   1.6123,   1.5618,   0.6190,  -0.3316,\n",
            "          -1.9285,  -4.0191],\n",
            "        [  0.7641,   1.6306,   2.2016,   1.6123,   1.5618,   0.6190,  -0.3316,\n",
            "          -1.9285,  -4.0191],\n",
            "        [  3.4472,   2.5669,   2.0166,   1.2797,  -0.2200,  -2.2143,  -4.6182,\n",
            "          -5.8231, -10.7848]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2240245158855731\n",
            "===================================\n",
            "===============Epoch390==============\n",
            "tensor([[  0.7573,   1.6210,   2.1909,   1.5996,   1.5495,   0.6068,  -0.3460,\n",
            "          -1.9396,  -4.0657],\n",
            "        [  3.0389,   2.3154,   2.0307,   1.0262,  -0.2250,  -2.8216,  -4.8657,\n",
            "          -6.1380, -10.0163],\n",
            "        [  0.7573,   1.6210,   2.1909,   1.5996,   1.5495,   0.6068,  -0.3460,\n",
            "          -1.9396,  -4.0657],\n",
            "        [  0.7573,   1.6210,   2.1909,   1.5996,   1.5495,   0.6068,  -0.3460,\n",
            "          -1.9396,  -4.0657],\n",
            "        [  3.4362,   2.5547,   2.0037,   1.2652,  -0.2343,  -2.2288,  -4.6370,\n",
            "          -5.8373, -10.9410]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2240213889342089\n",
            "===================================\n",
            "===============Epoch400==============\n",
            "tensor([[  0.7412,   1.6089,   2.1814,   1.5912,   1.5410,   0.6035,  -0.3432,\n",
            "          -1.9331,  -4.0762],\n",
            "        [  3.0253,   2.3041,   2.0211,   1.0179,  -0.2360,  -2.8397,  -4.8853,\n",
            "          -6.1703, -10.1397],\n",
            "        [  0.7412,   1.6089,   2.1814,   1.5912,   1.5410,   0.6035,  -0.3432,\n",
            "          -1.9331,  -4.0762],\n",
            "        [  0.7412,   1.6089,   2.1814,   1.5912,   1.5410,   0.6035,  -0.3432,\n",
            "          -1.9331,  -4.0762],\n",
            "        [  3.4202,   2.5407,   1.9914,   1.2536,  -0.2501,  -2.2442,  -4.6480,\n",
            "          -5.8422, -11.0541]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.223984800852262\n",
            "===================================\n",
            "===============Epoch410==============\n",
            "tensor([[  0.7355,   1.5998,   2.1712,   1.5793,   1.5298,   0.5907,  -0.3600,\n",
            "          -1.9466,  -4.1195],\n",
            "        [  3.0164,   2.2944,   2.0115,   1.0080,  -0.2441,  -2.8551,  -4.9113,\n",
            "          -6.2104, -10.2740],\n",
            "        [  0.7355,   1.5998,   2.1712,   1.5793,   1.5298,   0.5907,  -0.3600,\n",
            "          -1.9466,  -4.1195],\n",
            "        [  0.7355,   1.5998,   2.1712,   1.5793,   1.5298,   0.5907,  -0.3600,\n",
            "          -1.9466,  -4.1195],\n",
            "        [  3.4073,   2.5280,   1.9792,   1.2412,  -0.2601,  -2.2521,  -4.6609,\n",
            "          -5.8507, -11.1735]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.223994424709907\n",
            "===================================\n",
            "===============Epoch420==============\n",
            "tensor([[  0.7133,   1.5882,   2.1623,   1.5740,   1.5239,   0.5943,  -0.3518,\n",
            "          -1.9329,  -4.1098],\n",
            "        [  2.9980,   2.2845,   2.0023,   1.0024,  -0.2546,  -2.8682,  -4.9285,\n",
            "          -6.2382, -10.3625],\n",
            "        [  0.7133,   1.5882,   2.1623,   1.5740,   1.5239,   0.5943,  -0.3518,\n",
            "          -1.9329,  -4.1098],\n",
            "        [  0.7133,   1.5882,   2.1623,   1.5740,   1.5239,   0.5943,  -0.3518,\n",
            "          -1.9329,  -4.1097],\n",
            "        [  3.3861,   2.5157,   1.9674,   1.2327,  -0.2751,  -2.2637,  -4.6712,\n",
            "          -5.8523, -11.2480]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.22394813482578\n",
            "===================================\n",
            "===============Epoch430==============\n",
            "tensor([[  0.7095,   1.5785,   2.1531,   1.5623,   1.5132,   0.5807,  -0.3633,\n",
            "          -1.9424,  -4.1418],\n",
            "        [  2.9932,   2.2741,   1.9934,   0.9914,  -0.2640,  -2.8877,  -4.9511,\n",
            "          -6.2763, -10.4709],\n",
            "        [  0.7095,   1.5785,   2.1531,   1.5623,   1.5132,   0.5807,  -0.3633,\n",
            "          -1.9424,  -4.1418],\n",
            "        [  0.7095,   1.5785,   2.1531,   1.5623,   1.5132,   0.5807,  -0.3633,\n",
            "          -1.9424,  -4.1418],\n",
            "        [  3.3797,   2.5027,   1.9561,   1.2184,  -0.2880,  -2.2810,  -4.6859,\n",
            "          -5.8640, -11.3432]], grad_fn=<SliceBackward0>)\n",
            "tensor([2., 0., 2., 2., 0.])\n",
            "tensor([0, 0, 2, 4, 0])\n",
            "train acc: tensor(0.5167)\n",
            "test acc: tensor(0.5224)\n",
            "val acc: tensor(0.5148)\n",
            "train loss: 1.2239436782323396\n",
            "===================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c31e46f6a95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-1c445a4ae179>\u001b[0m in \u001b[0;36meval\u001b[0;34m(dataset, model, loss_fn, optimiser, max_patience, num_splits, batch_size, epochs, output_every, early_stop)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtemp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     74\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Collate attributes into a unified representation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             value, slices, incs = _collate(attr, values, data_list, stores,\n\u001b[0m\u001b[1;32m     85\u001b[0m                                            increment)\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat_dim\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mincrement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mincs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_incs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 values = [\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mget_incs\u001b[0;34m(key, values, data_list, stores)\u001b[0m\n\u001b[1;32m    260\u001b[0m def get_incs(key, values: List[Any], data_list: List[BaseData],\n\u001b[1;32m    261\u001b[0m              stores: List[BaseStorage]) -> Tensor:\n\u001b[0;32m--> 262\u001b[0;31m     repeats = [\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m              stores: List[BaseStorage]) -> Tensor:\n\u001b[1;32m    262\u001b[0m     repeats = [\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = GCN_N(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN_MAX(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ],
      "metadata": {
        "id": "F_X2Ufpu1PwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN_CAT(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ],
      "metadata": {
        "id": "y_-hi1x51Qih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT_N(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ],
      "metadata": {
        "id": "zbOOLMS11TOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT_MAX(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ],
      "metadata": {
        "id": "HQPL_n3C1T3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT_CAT(input_dim=1, hidden_dim=9, output_dim=9, num_layers=3).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr=0.01, params=model.parameters())\n",
        "eval(dataset, model1, loss_fn, optimizer, epochs=400, output_every=10, batch_size=1024)"
      ],
      "metadata": {
        "id": "KfFQ-4Vq1UFG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}